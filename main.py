import time
from detect_hw import detect_compute_devices
from compute_info import get_gpu_utilization_fast, luid_to_int
from get_dgpu_usage import get_dgpu_utilization_nvidia_smi, get_dgpu_vram
from benchmark_final import auto_find_threshold

def get_igpu_npu_usage():
    """ä½¿ç”¨å¿«é€Ÿç‰ˆæœ¬ç²å– iGPU å’Œ NPU çš„ä½¿ç”¨ç‡èˆ‡è¨˜æ†¶é«” (MB)

    å›å‚³: (igpu_util, npu_util, igpu_mem_MB, npu_mem_MB)
    """
    igpu_util = npu_util = 0.0
    igpu_mem = npu_mem = 0.0

    try:
        luid_utilization_data = get_gpu_utilization_fast()
        if not luid_utilization_data:
            print("âš ï¸ ç„¡æ³•å–å¾— GPU/NPU ä½¿ç”¨ç‡è³‡æ–™ã€‚")
            return igpu_util, npu_util, igpu_mem, npu_mem

        # æŒ‰ LUID æ’åº
        luid_utilization_data.sort(key=lambda d: luid_to_int(d["luid"]))

        # æœ€å° LUID â†’ iGPU
        igpu_util = luid_utilization_data[0]["utilization"]
        igpu_mem = luid_utilization_data[0].get("memory_usage_MB", 0.0)

        # æœ€å¤§ LUID â†’ NPUï¼ˆå¦‚æœæœ‰å¤šå€‹è¨­å‚™ï¼‰
        if len(luid_utilization_data) > 1:
            npu_util = luid_utilization_data[-1]["utilization"]
            npu_mem = luid_utilization_data[-1].get("memory_usage_MB", 0.0)

    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å–å¾—ä½¿ç”¨ç‡è³‡è¨Š: {e}")

    return igpu_util, npu_util, igpu_mem, npu_mem
def pick_best_dgpu_model(dgpu_mem, model_list, model_vram):
    """
    æ ¹æ“š dGPU å¯ç”¨ VRAM é¸æ“‡æœ€ä½³æ¨¡å‹

    åƒæ•¸ï¼š
        dgpu_mem (float): dGPU å¯ç”¨ VRAM (GB)
        model_list (dict): å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œä¾‹å¦‚ {"dGPU": ["model1", "model2"]}
        model_vram (dict): æ¯å€‹æ¨¡å‹æ‰€éœ€ VRAMï¼Œä¾‹å¦‚ {"model1": 6, "model2": 12}

    å›å‚³ï¼š
        str æˆ– Noneï¼šé¸åˆ°çš„æœ€ä½³æ¨¡å‹åç¨±ï¼Œè‹¥ç„¡å¯ç”¨æ¨¡å‹å›å‚³ None
    """

    candidates = model_list.get("dGPU", [])

    # éæ¿¾ VRAM è¶³å¤ çš„æ¨¡å‹
    available = [
        model for model in candidates
        if model_vram.get(model, 0) <= dgpu_mem
    ]

    if not available:
        return None  # VRAM ä¸å¤ ï¼Œä¸èƒ½ç”¨ dGPU

    # é¸ VRAM éœ€æ±‚æœ€å¤§çš„æ¨¡å‹
    return sorted(
        available,
        key=lambda m: model_vram[m],
        reverse=True
    )[0]

def select_best_device_and_model(devices, igpu_util, npu_util, dgpu_util, dgpu_mem, usage_threshold, model_list, model_vram):
    """
    é¸æ“‡æœ€ä½³é‹ç®—è£ç½®ä¸¦å›å‚³å°æ‡‰çš„æ¨¡å‹

    å‡½å¼ç”¨é€”ï¼š
        æ ¹æ“šç³»çµ±ä¸­ dGPUã€iGPUã€NPU çš„å­˜åœ¨ç‹€æ…‹èˆ‡ä½¿ç”¨ç‡ï¼Œä»¥åŠ dGPU å¯ç”¨ VRAMï¼Œ
        è‡ªå‹•é¸æ“‡æœ€é©åˆåŸ·è¡Œ AI æ¨¡å‹çš„é‹ç®—è£ç½®ï¼Œä¸¦å›å‚³å°æ‡‰çš„æ¨¡å‹åç¨±ã€‚

    åƒæ•¸ï¼š
        devices (dict): åµæ¸¬åˆ°çš„ç¡¬é«”è¨­å‚™
            ä¾‹å¦‚ {"dGPU": True, "iGPU": True, "NPU": True}
        igpu_util (float): iGPU ç•¶å‰ä½¿ç”¨ç‡ (0~100)
        npu_util (float): NPU ç•¶å‰ä½¿ç”¨ç‡ (0~100)
        dgpu_util (float): dGPU ç•¶å‰ä½¿ç”¨ç‡ (0~100)
        dgpu_mem (float): dGPU å¯ç”¨ VRAM (GB)
        usage_threshold (float): iGPU / NPU æœ€å¤§å¯æ¥å—ä½¿ç”¨ç‡é–€æª» (0~1ï¼Œä¾‹å¦‚ 0.5 è¡¨ç¤º 50%)

    å›å‚³ï¼š
        tuple (str, str): (é¸æ“‡çš„è£ç½®åç¨±, å°æ‡‰æ¨¡å‹åç¨±)
            è£ç½®åç¨±å¯èƒ½ç‚º "dGPU", "iGPU", "NPU"
            æ¨¡å‹åç¨±æ ¹æ“šè£ç½®å’Œå¯ç”¨ VRAM é¸æ“‡

    æµç¨‹èªªæ˜ï¼š
        1. é¡¯ç¤ºç›®å‰åµæ¸¬åˆ°çš„ç¡¬é«”èˆ‡ä½¿ç”¨ç‡ã€‚
        2. å„ªå…ˆä½¿ç”¨ dGPUï¼š
            - è‹¥ dGPU ä½¿ç”¨ç‡ â‰¤ 50%ï¼Œå‘¼å« pick_best_dgpu_model() é¸å‡º VRAM è¶³å¤ ä¸”éœ€æ±‚æœ€å¤§çš„æ¨¡å‹ã€‚
            - è‹¥ VRAM ä¸è¶³æˆ–ä½¿ç”¨ç‡éé«˜ï¼Œè·³é dGPUã€‚
        3. åˆ¤æ–· iGPUï¼š
            - è‹¥ iGPU å­˜åœ¨ä¸”ä½¿ç”¨ç‡ â‰¤ usage_thresholdï¼Œä½¿ç”¨ iGPU å°æ‡‰æ¨¡å‹ã€‚
        4. åˆ¤æ–· NPUï¼š
            - è‹¥ NPU å­˜åœ¨ä¸”ä½¿ç”¨ç‡ â‰¤ usage_thresholdï¼Œä½¿ç”¨ NPU å°æ‡‰æ¨¡å‹ã€‚
        5. fallbackï¼š
            - è‹¥æ‰€æœ‰è£ç½®éƒ½è¶…è¼‰æˆ–ç„¡å¯ç”¨æ¨¡å‹ï¼Œé è¨­ä½¿ç”¨ iGPU åŠå…¶æ¨¡å‹ã€‚
    """
    
    print("=== åµæ¸¬åˆ°çš„ç¡¬é«” ===")
    # 1. dGPU å„ªå…ˆåˆ¤æ–·
    if devices.get("dGPU", False):
        print(f"â¡ï¸ dGPU ä½¿ç”¨ç‡: {dgpu_util:.2f}% VRAM: {dgpu_mem:.2f}GB")

        if dgpu_util <= 50.0:
            model = pick_best_dgpu_model(dgpu_mem, model_list, model_vram)

            if model:
                print(f"âœ… dGPU VRAM è¶³å¤ ï¼Œé¸æ“‡æ¨¡å‹: {model}")
                return "dGPU", model
            else:
                print("âš ï¸ dGPU VRAM ä¸è¶³ï¼Œè·³é dGPU")
        else:
            print("âš ï¸ dGPU ä½¿ç”¨ç‡éé«˜ï¼Œè·³é dGPU")
    # 2. iGPU
    if devices.get("iGPU", False) and igpu_util <= usage_threshold * 100:
        model = MODEL_LIST["iGPU"][0]
        print(f"â¡ï¸ iGPU ä½¿ç”¨ç‡ OKï¼Œä½¿ç”¨ {model}")
        return "iGPU", model
    # 3. NPU
    if devices.get("NPU", False) and npu_util <= usage_threshold * 100:
        model = MODEL_LIST["NPU"][0]
        print(f"â¡ï¸ NPU ä½¿ç”¨ç‡ OKï¼Œä½¿ç”¨ {model}")
        return "NPU", model
    # 4. fallback
    print("âš ï¸ å…¨éƒ¨è£ç½®éƒ½ç¹å¿™ï¼Œfallback è‡³ iGPU")
    return "iGPU", MODEL_LIST["iGPU"][0]




if __name__ == "__main__":
    print("=== æ™ºèƒ½è£ç½®é¸æ“‡ç³»çµ±å•Ÿå‹• ===")
    
    # â¬…ï¸ åˆå§‹åŒ–ï¼šåªåµæ¸¬ä¸€æ¬¡ç¡¬é«”
    devices = detect_compute_devices()
    # if devices['iGPU'] is True and devices['NPU'] is True:
        # usage = auto_find_threshold("Qwen3-8B-int4-cw-ov", "Qwen3-8B-int4-ov")
    MODEL_LIST = {
    "dGPU": ["gpt-oss:20b", "qwen3:14b", "qwen3:8b"],
    "iGPU": ["OpenVINO/Qwen3-8B-int4-ov"],
    "NPU":  ["OpenVINO/Qwen3-8B-int4-cw-ov"]
    }
    MODEL_VRAM = {
    "gpt-oss:20b": 15,
    "qwen3:14b": 12,
    "qwen3:8b": 6,
    "OpenVINO/Qwen3-8B-int4-ov": 0,
    "OpenVINO/Qwen3-8B-int4-cw-ov": 0
    }
    while True:
        # ç²å–å„è£ç½®çš„ä½¿ç”¨ç‡
        # é è¨­ç‚º 0.0ï¼ˆè‹¥æ²’æœ‰ dGPU æˆ–ç„¡æ³•å–å¾—å‰‡ç¶­æŒ 0ï¼‰
        dgpu_util = 0.0
        dgpu_util_vram = 0.0
        usage = 0.0
        if devices.get("dGPU", False):
            try:
                dgpu_util = get_dgpu_utilization_nvidia_smi()
                dgpu_util_vram = get_dgpu_vram()
                print(f"NVIDIA dGPU VRAM ä½¿ç”¨é‡: {dgpu_util_vram:.2f} GB ")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•å–å¾— dGPU ä½¿ç”¨ç‡: {e}")
                dgpu_util = 0.0
        print("=== å–å¾—å„è£ç½®ä½¿ç”¨ç‡ ===")
        # igpu_util, npu_util, igpu_mem, npu_mem = get_igpu_npu_usage()
        igpu_util = 51
        npu_util = 25
        igpu_mem = 0.0
        dgpu_util = 51
        # print(f"ğŸ® iGPU ä½¿ç”¨ç‡: {igpu_util:.2f}%, è¨˜æ†¶é«”ä½¿ç”¨: {igpu_mem:.2f} MB")
        best, model = select_best_device_and_model(devices, igpu_util, npu_util, dgpu_util, dgpu_util_vram ,0.5, MODEL_LIST, MODEL_VRAM)
        print(f"å»ºè­°ä½¿ç”¨è£ç½®: {best}, æ¨¡å‹: {model}")
        time.sleep(10)


# import time
# from detect_hw import detect_compute_devices
# from compute_info import get_gpu_utilization_fast, luid_to_int
# from get_dgpu_usage import get_dgpu_utilization_nvidia_smi, get_dgpu_vram
# from benchmark_final import auto_find_threshold

# def get_igpu_npu_usage():
#     """å¿«é€Ÿç²å– iGPU å’Œ NPU çš„ä½¿ç”¨ç‡èˆ‡è¨˜æ†¶é«” (MB)"""
#     igpu_util = npu_util = 0.0
#     igpu_mem = npu_mem = 0.0
#     try:
#         luid_utilization_data = get_gpu_utilization_fast()
#         if not luid_utilization_data:
#             print("âš ï¸ ç„¡æ³•å–å¾— GPU/NPU ä½¿ç”¨ç‡è³‡æ–™ã€‚")
#             return igpu_util, npu_util, igpu_mem, npu_mem

#         luid_utilization_data.sort(key=lambda d: luid_to_int(d["luid"]))

#         # æœ€å° LUID â†’ iGPU
#         igpu_util = luid_utilization_data[0]["utilization"]
#         igpu_mem = luid_utilization_data[0].get("memory_usage_MB", 0.0)

#         # æœ€å¤§ LUID â†’ NPU
#         if len(luid_utilization_data) > 1:
#             npu_util = luid_utilization_data[-1]["utilization"]
#             npu_mem = luid_utilization_data[-1].get("memory_usage_MB", 0.0)

#     except Exception as e:
#         print(f"âš ï¸ ç„¡æ³•å–å¾—ä½¿ç”¨ç‡è³‡è¨Š: {e}")

#     return igpu_util, npu_util, igpu_mem, npu_mem


# def select_best_device_and_model(devices, igpu_util=0.0, npu_util=0.0, dgpu_util=0.0, dgpu_mem=0.0, usage=0.0):
#     """é¸æ“‡æœ€ä½³é‹ç®—è£ç½®èˆ‡å°æ‡‰æ¨¡å‹"""
#     device_model_mapping = {
#         "dGPU": "gpt-oss:20b",
#         "iGPU": "OpenVINO/Qwen3-8B-int4-ov",
#         "NPU": "OpenVINO/Qwen3-8B-int4-cw-ov"
#     }

#     selected_device = "iGPU"

#     # Step 1 - dGPU
#     if devices.get("dGPU", False):
#         if dgpu_util <= 50.0 or dgpu_mem <= 6.0:
#             selected_device = "dGPU"
#             if 6.0 <= dgpu_mem <= 11.5:
#                 device_model_mapping[selected_device] = "qwen3:8b"
#             elif 11.5 < dgpu_mem <= 13.5:
#                 device_model_mapping[selected_device] = "qwen3:14b"
#             else:
#                 device_model_mapping[selected_device] = "gpt-oss:20b"
#             return selected_device, device_model_mapping[selected_device]

#     # Step 2 & 3 - iGPU / NPU
#     if devices.get("iGPU", False) and igpu_util <= usage * 100:
#         selected_device = "iGPU"
#         return selected_device, device_model_mapping[selected_device]

#     if devices.get("NPU", False) and npu_util <= usage * 100:
#         selected_device = "NPU"
#         return selected_device, device_model_mapping[selected_device]

#     # ç„¡ä½è² è¼‰è£ç½® â†’ é è¨­ iGPU
#     return selected_device, device_model_mapping[selected_device]


# def select_optimal_device_loop(loop_interval=1):
#     """
#     æ•´åˆæµç¨‹ï¼šåµæ¸¬ç¡¬é«” â†’ å–å¾—ä½¿ç”¨ç‡ â†’ é¸æ“‡æœ€ä½³è£ç½® â†’ å¾ªç’°è¼¸å‡ºå»ºè­°
    
#     åƒæ•¸ï¼š
#         loop_interval (int): æ¯æ¬¡è¿´åœˆé–“éš”ç§’æ•¸
#     """
#     print("=== æ™ºèƒ½è£ç½®é¸æ“‡ç³»çµ±å•Ÿå‹• ===")
    
#     # åµæ¸¬ç¡¬é«”
#     devices = detect_compute_devices()
    
#     # è‡ªå‹•é–¾å€¼è¨­å®šï¼ˆiGPU/NPU ç”¨ï¼‰
#     usage = 0.0
#     if devices.get('iGPU', False) and devices.get('NPU', False):
#         usage = auto_find_threshold("Qwen3-8B-int4-cw-ov", "Qwen3-8B-int4-ov")

#     while True:
#         # é è¨­ dGPU ä½¿ç”¨ç‡/é¡¯å­˜
#         dgpu_util = dgpu_util_vram = 0.0

#         if devices.get("dGPU", False):
#             try:
#                 dgpu_util = get_dgpu_utilization_nvidia_smi()
#                 dgpu_util_vram = get_dgpu_vram()
#             except Exception as e:
#                 print(f"âš ï¸ ç„¡æ³•å–å¾— dGPU ä½¿ç”¨ç‡: {e}")

#         # å–å¾— iGPU / NPU ä½¿ç”¨ç‡
#         igpu_util, npu_util, igpu_mem, npu_mem = get_igpu_npu_usage()

#         # é¸æ“‡æœ€ä½³è£ç½®èˆ‡æ¨¡å‹
#         best_device, model = select_best_device_and_model(
#             devices, igpu_util, npu_util, dgpu_util, dgpu_util_vram, usage
#         )

#         # è¼¸å‡ºè³‡è¨Š
#         print("=== å–å¾—å„è£ç½®ä½¿ç”¨ç‡ ===")
#         print(f"ğŸ’¾ dGPU ä½¿ç”¨ç‡: {dgpu_util:.2f}%, VRAM: {dgpu_util_vram:.2f} GB")
#         print(f"ğŸ® iGPU ä½¿ç”¨ç‡: {igpu_util:.2f}%, è¨˜æ†¶é«”ä½¿ç”¨: {igpu_mem:.2f} MB")
#         print(f"âš™ï¸ NPU ä½¿ç”¨ç‡: {npu_util:.2f}%, è¨˜æ†¶é«”ä½¿ç”¨: {npu_mem:.2f} MB")
#         print(f"âœ… å»ºè­°ä½¿ç”¨è£ç½®: {best_device}, æ¨¡å‹: {model}\n")

#         time.sleep(loop_interval)


# # ç¯„ä¾‹ï¼šå•Ÿå‹•æµç¨‹
# if __name__ == "__main__":
#     select_optimal_device_loop(loop_interval=1)
