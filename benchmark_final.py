import time
import requests
import numpy as np
import subprocess
import sys
PORT = 8000  # ä½ ç”¨å–®ä¸€ OVMS port

models = [
    ("Qwen3-4B-int4-ov", "Qwen3-4B-int4-cw-ov"),  # (NPU_model, iGPU_model)
    ("Qwen3-8B-int4-ov", "Qwen3-8B-int4-cw-ov")
]

prompt = "Hello, how are you today?"
tokens_to_generate = 1000

# --------------------------------------------------
# å‘¼å« usage_load.py ä¾†å•Ÿå‹•æŒ‡å®šçš„ iGPU ä½¿ç”¨ç‡
# --------------------------------------------------
def start_load_process(load_value):
    print(f"\nâš™ï¸ å•Ÿå‹• iGPU load = {load_value:.2f}")

    # é–‹æ–° subprocess è®“å®ƒè‡ªå·±è·‘ï¼Œä¸é˜»å¡ä¸»ç¨‹å¼
    p = subprocess.Popen(
    [sys.executable, "usage_load.py", "--load", str(load_value)],
    )


    # çµ¦ 3 ç§’é˜è®“è² è¼‰ç©©å®š
    time.sleep(3)
    return p


# --------------------------------------------------
# åœæ­¢ usage_load.py
# --------------------------------------------------
def stop_load_process(proc):
    print("ğŸ›‘ åœæ­¢è² è¼‰ç¨‹å¼")
    proc.terminate()
    time.sleep(1)


def benchmark_ovms(model_name, prompt, max_tokens):
    url = f"http://localhost:{PORT}/v3/chat/completions"

    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ],
        "max_new_tokens": max_tokens,
        "temperature": 0
    }

    start = time.time()
    response = requests.post(url, json=payload)
    end = time.time()

    if response.status_code != 200:
        print(f"âŒ Error: {response.text}")
        return None, 0

    data = response.json()
    actual_tokens = data.get("usage", {}).get("completion_tokens", None)

    if actual_tokens is None:
        print("âš ï¸ OVMS æ²’å›å‚³ usage")
        return None, 0

    total_time = end - start
    tps = actual_tokens / total_time
    return total_time, tps


def run_benchmark(model_npu, model_igpu, prompt, tokens):
    print("\n============================================")
    print(f"ğŸ§ª Benchmark: {model_npu} vs {model_igpu}")
    print("============================================\n")

    # NPU
    print("âš¡ Testing NPU...")
    t_npu, tps_npu = benchmark_ovms(model_npu, prompt, tokens)

    # iGPU
    print("âš¡ Testing iGPU...")
    t_igpu, tps_igpu = benchmark_ovms(model_igpu, prompt, tokens)

    print("\n=== Result ===")
    print(f"NPU  ({model_npu}):  {tps_npu:.2f} tok/s")
    print(f"iGPU ({model_igpu}): {tps_igpu:.2f} tok/s")
    print("-------------------------------")

    if tps_igpu > tps_npu:
        print("ğŸ† iGPU is faster")
    else:
        print("ğŸ† NPU is faster")

    return tps_npu, tps_igpu


def auto_find_threshold(model_npu, model_igpu):
    print("\n============================================")
    print(f"ğŸ” Auto threshold test for {model_igpu}")
    print("============================================")

    test_loads = np.linspace(0.3, 1.0, 8)

    for load in test_loads:
        # print(f"\nâš™ï¸ è«‹æ‰‹å‹•åŸ·è¡Œï¼špython burn_load.py --load {load:.1f}")
        # input("ğŸ‘‰ æŒ‰ Enter ç¹¼çºŒè·‘ benchmark...")
        procs = start_load_process(load)
        tps_npu, tps_igpu = run_benchmark(model_npu, model_igpu, prompt, tokens_to_generate)
        stop_load_process(procs)
        if tps_igpu < tps_npu:
            print(f"\nğŸ“Œ å»ºè­°åˆ‡æ›é»ï¼šCPU/iGPU load > {load*100:.0f}% â†’ æ› NPU")
            return load
            break


if __name__ == "__main__":
    for model_igpu, model_npu in models:
        start_time = time.time()
        usage = auto_find_threshold(model_npu, model_igpu)
        print(f"âš™ï¸ å»ºè­° iGPU ä½¿ç”¨ç‡åˆ‡æ›é»: {usage*100:.0f}%")
        end_time = time.time()
        elapsed = end_time - start_time
        print(f"â±ï¸ {model_npu} , {model_igpu}æ¸¬è©¦å®Œæˆï¼Œè€—æ™‚ {elapsed:.2f} ç§’\n")
