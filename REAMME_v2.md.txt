
https://docs.openvino.ai/2025/get-started/install-openvino.html?PACKAGE=OPENVINO_GENAI&VERSION=v_2025_3_0&OP_SYSTEM=WINDOWS&DISTRIBUTION=PIP

# Step 1: Create virtual environment
python -m venv openvino_env

# Step 2: Activate virtual environment
openvino_env\Scripts\activate

# Step 3: Upgrade pip to latest version
python -m pip install --upgrade pip

# Step 4: Download and install the package
pip install openvino-genai==2025.3.0

===================
# https://docs.openvino.ai/2025/model-server/ovms_docs_deploying_server_baremetal.html
# Step 1:下載 OVMS 

curl -L https://github.com/openvinotoolkit/model_server/releases/download/v2025.3/ovms_windows_python_on.zip -o ovms.zip

# Step 2:解壓縮
tar -xf ovms.zip

# Step 3:將ovms資料夾加入環境變數

# Step 4:下載並安裝 python 3.12.10

# Step 5:python 設定環境變數
將 C:\Users\<Username>\AppData\Local\Programs\Python\Python312\Scripts\ 加入環境變數，並移動至最上方
將 C:\Users\zpdx11\AppData\Local\Programs\Python\Python312\ 加入環境變數，並移動至第二個

==================

下載模型 https://docs.openvino.ai/2025/model-server/ovms_demos_continuous_batching_agent.html#direct-pulling-of-pre-configured-huggingface-models-on-windows

範例:
ovms.exe --pull --model_repository_path models --source_model OpenVINO/Phi-3.5-mini-instruct-int4-cw-ov --task text_generation

=================
啟動ovms server: https://docs.openvino.ai/2025/model-server/ovms_demos_continuous_batching_agent.html#deploying-on-windows-with-gpu

ovms.exe --rest_port 8000 --source_model OpenVINO/Qwen3-8B-int4-cw-ov  --model_repository_path models --target_device GPU --cache_size 2 --task text_generation --max_num_batched_tokens 99999

ovms.exe --rest_port 8000 --source_model OpenVINO/Phi-3-medium-4k-instruct-int4-ov  --model_repository_path models --target_device GPU --cache_size 2 --task text_generation --max_num_batched_tokens 99999

ovms.exe --rest_port 8001 --source_model OpenVINO/Phi-3.5-mini-instruct-int4-cw-ov --model_repository_path models --target_device NPU --cache_size 2 --task text_generation --max_num_batched_tokens 99999

ovms.exe --rest_port 8000 --source_model OpenVINO/Mistral-7B-Instruct-v0.3-int4-cw-ov --model_repository_path models --tool_parser mistral --target_device GPU --cache_size 2 --task text_generation --max_num_batched_tokens 99999
=================
測試是否可連接ovms server
一次啟動好幾個model model_path要拿掉前面的OpenVINO
curl http://localhost:8000/v3/chat/completions -H "Content-Type: application/json" -d "{\"model\":\"OpenVINO/Qwen3-8B-int4-cw-ov\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},{\"role\":\"user\",\"content\":\"Say this is a test\"}]}"

curl -w "\nTime: %{time_total}s\n" -o NUL -s ^
-H "Content-Type: application/json" ^
-d "{\"model\":\"OpenVINO/Qwen3-8B-int4-ov\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},{\"role\":\"user\",\"content\":\"Say this is a test\"}]}" ^
http://localhost:8000/v3/chat/completions


=================
新增config
ovms --model_name DeepSeek-R1-Distill-Qwen-1.5B-int4-cw-ov --model_path C:\Users\zpdx11\Desktop\ov\models\OpenVINO\DeepSeek-R1-Distill-Qwen-1.5B-int4-cw-ov --add_to_config C:\Users\zpdx11\Desktop\ov\config.json

================
執行config
ovms --config_path C:\\Users\\zpdx11\\Desktop\\ov\\config.json --rest_port 8000


Get-Counter "\GPU Engine(*)\Utilization Percentage" | Select-Object -ExpandProperty CounterSamples | Where-Object {$_.InstanceName -match "engtype_3D"} | Select InstanceName, CookedValue
